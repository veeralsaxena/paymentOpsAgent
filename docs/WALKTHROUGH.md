# üß† Payment Operations Agentic AI System - Complete Walkthrough

> **Team B042** | Veeral Saxena & Srishtee Varule | Taqneeq Hackathon 2026

This document provides a detailed technical explanation of how the entire Payment Operations Agent system works.

---

## ‚úÖ Hackathon Requirements Compliance

| Requirement                                                    | How We Satisfy It                                                                                                                |
| -------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| **Real agent logic (state, memory, tools, decision policies)** | ‚úÖ LangGraph StateGraph with typed AgentState, vector memory in PostgreSQL+pgvector, 10+ tools, contextual bandit policy learner |
| **Not just a single LLM call**                                 | ‚úÖ 5-stage loop (Observe‚ÜíReason‚ÜíDecide‚ÜíAct‚ÜíLearn), ML models (Isolation Forest, XGBoost), policy learner                         |
| **Payment data ingestion**                                     | ‚úÖ Simulator generates transactions ‚Üí Redis Streams ‚Üí Agent observes                                                             |
| **How decisions are made**                                     | ‚úÖ Contextual Bandit policy predicts utility for each action, picks highest                                                      |
| **How actions are executed**                                   | ‚úÖ Tools execute via real function calls (switch_gateway, adjust_retry)                                                          |
| **Outcomes feed back into reasoning**                          | ‚úÖ Reward calculation ‚Üí Policy update ‚Üí Memory storage                                                                           |

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         FRONTEND (Next.js 15 + Tailwind)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Real-time     ‚îÇ  ‚îÇ Bank Health   ‚îÇ  ‚îÇ Agent Thought ‚îÇ  ‚îÇ Scenario          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ Metrics       ‚îÇ  ‚îÇ Grid          ‚îÇ  ‚îÇ Stream        ‚îÇ  ‚îÇ Injection         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ WebSocket (Real-time bidirectional)
                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BACKEND (Python FastAPI + LangGraph)                          ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                    LangGraph Agent (State Machine)                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îÇ OBSERVE ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ REASON  ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ DECIDE  ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ   ACT   ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÇ  LEARN  ‚îÇ    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Get Metrics   Gemini 2.0    Policy       Execute       Calculate       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Detect        Analysis      Learner      Tools         Reward          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   Anomalies                   (Bandit)                   Update Policy   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Simulator       ‚îÇ  ‚îÇ ML Layer        ‚îÇ  ‚îÇ Agent Tools                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Service         ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ  ‚îÇ ‚Ä¢ get_current_metrics          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Generates       ‚îÇ  ‚îÇ ‚Ä¢ Anomaly       ‚îÇ  ‚îÇ ‚Ä¢ get_bank_status              ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ transactions    ‚îÇ  ‚îÇ   Detector      ‚îÇ  ‚îÇ ‚Ä¢ switch_gateway               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ (50 TPS)        ‚îÇ  ‚îÇ   (Isolation    ‚îÇ  ‚îÇ ‚Ä¢ adjust_retry_config          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   Forest)       ‚îÇ  ‚îÇ ‚Ä¢ send_alert                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Injects         ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ ‚Ä¢ store_memory                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ failures        ‚îÇ  ‚îÇ ‚Ä¢ Failure       ‚îÇ  ‚îÇ ‚Ä¢ recall_similar_patterns      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ on command      ‚îÇ  ‚îÇ   Predictor     ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   (XGBoost)     ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ ‚Ä¢ Policy        ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   Learner       ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   (Contextual   ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ   Bandit)       ‚îÇ  ‚îÇ                                ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                   ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚ñº                       ‚ñº                       ‚ñº
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ     Redis       ‚îÇ    ‚îÇ   PostgreSQL    ‚îÇ    ‚îÇ     Gemini      ‚îÇ
   ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    ‚îÇ   2.0 Flash     ‚îÇ
   ‚îÇ ‚Ä¢ Real-time    ‚îÇ    ‚îÇ ‚Ä¢ Agent Memory ‚îÇ    ‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚îÇ
   ‚îÇ   Metrics      ‚îÇ    ‚îÇ   (pgvector)   ‚îÇ    ‚îÇ ‚Ä¢ Reasoning     ‚îÇ
   ‚îÇ ‚Ä¢ Transaction  ‚îÇ    ‚îÇ ‚Ä¢ Intervention ‚îÇ    ‚îÇ ‚Ä¢ Hypothesis    ‚îÇ
   ‚îÇ   Stream       ‚îÇ    ‚îÇ   History      ‚îÇ    ‚îÇ   Formation     ‚îÇ
   ‚îÇ ‚Ä¢ Bank Health  ‚îÇ    ‚îÇ                ‚îÇ    ‚îÇ                 ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Payment Flow: How Payments Happen

### 1. Transaction Generation (Simulator Service)

**Location:** `backend/app/services/simulator_service.py`

```python
# Banks with weighted traffic distribution
self.banks = {
    "HDFC": {"weight": 40, ...},  # 40% of traffic
    "ICICI": {"weight": 30, ...}, # 30% of traffic
    "SBI": {"weight": 20, ...},   # 20% of traffic
    "AXIS": {"weight": 10, ...}   # 10% of traffic
}

# Payment methods
self.payment_methods = {
    "visa": {"weight": 35, ...},
    "mastercard": {"weight": 30, ...},
    "upi": {"weight": 25, ...},
    "rupay": {"weight": 10, ...}
}
```

**How it works:**

1. Generates **50 transactions per second** (configurable)
2. Each transaction: bank, payment method, amount, success/failure
3. Base success rate: **97.5%**, Base latency: **200ms**
4. Pushes transactions to **Redis Streams**
5. Calculates real-time metrics and bank health

### 2. Failure Injection

When you click "Inject Failure" for a bank:

1. API call: `POST /api/simulator/scenario/custom`
2. Applies modifiers to the target bank:
   - `success_modifier`: Reduces success rate (e.g., -30%)
   - `latency_modifier`: Increases latency (e.g., +800ms)
3. Duration-limited (auto-clears after timeout)

---

## ü§ñ Agent Loop: The 5-Stage Decision Cycle

**Location:** `backend/app/agent/graph.py`

The agent runs continuously, executing every **5 seconds**:

```python
while self.is_running:
    initial_state = AgentState(metrics={}, anomalies=[], ...)
    await self.graph.astream(initial_state)  # Run state machine
    await asyncio.sleep(5)  # Wait 5 seconds
```

---

### Stage 1: OBSERVE üì°

**Purpose:** Ingest current system state

```python
async def _observe_node(self, state: AgentState) -> dict:
    # 1. Get current metrics from Redis
    metrics = await self.tools.get_current_metrics()
    # ‚Üí {success_rate: 97.5, avg_latency: 200, error_rate: 2.5}

    # 2. Get bank health status
    bank_health = await self.tools.get_bank_status()
    # ‚Üí [{name: "HDFC", success_rate: 98, status: "healthy"}, ...]

    # 3. Get recent error logs
    error_logs = await self.tools.get_error_logs(limit=100)

    # 4. Run ANOMALY DETECTION (Isolation Forest)
    anomalies = await self.tools.detect_anomalies(metrics)
    # ‚Üí ["success_rate_drop", "latency_spike", "error_rate_spike"]
```

**ML Component - Isolation Forest:**

- Trained on historical metrics patterns
- Detects "outlier" data points
- Thresholds: success_rate < 95%, latency > 350ms

---

### Stage 2: REASON üß†

**Purpose:** Analyze patterns using LLM + ML

```python
async def _reason_node(self, state: AgentState) -> dict:
    # 1. Get ML failure predictions (XGBoost)
    predictions = await self.tools.get_failure_predictions()
    # ‚Üí {"HDFC": {"risk": 0.8, "reason": "Latency spike (+0.4)"}}

    # 2. Recall similar past patterns (Vector Memory)
    memories = await self.tools.recall_similar_patterns(...)
    # ‚Üí [{"intervention": "switch_gateway", "outcome": "success"}]

    # 3. Call Gemini 2.0 Flash for reasoning
    response = await self.model.generate_content(prompt)
    # ‚Üí {hypothesis: "HDFC timeout due to load", severity: 0.7}

    # 4. If XGBoost predicts high risk, boost severity
    if max_ml_risk > 0.6:
        risk_score = max(risk_score, max_ml_risk)
```

---

### Stage 3: DECIDE ‚öñÔ∏è

**Purpose:** Choose best action using Policy Learner

```python
async def _decide_node(self, state: AgentState) -> dict:
    # 1. Define candidate actions
    candidate_actions = [
        {"id": "monitor", "action": "increase_monitoring"},
        {"id": "retry", "action": "adjust_retry_config"},
        {"id": "switch_gateway", "action": "switch_gateway"},
        {"id": "alert", "action": "send_alert"}
    ]

    # 2. Query Contextual Bandit for each action's utility
    for action in candidate_actions:
        utility = self.tools.ml.policy.predict_utility(context, action["id"])
        # ‚Üí monitor: U=0.1, retry: U=0.5, switch_gateway: U=1.3

    # 3. Select action with highest utility
    best_action = max(actions, key=lambda a: a.utility)
```

---

### Stage 4: ACT üöÄ

**Purpose:** Execute the intervention

```python
async def _act_node(self, state: AgentState) -> dict:
    intervention = state["intervention"]

    if action == "switch_gateway":
        success = await self.tools.switch_gateway(
            from_bank="HDFC",   # Failing bank
            to_bank="ICICI",    # Healthy alternative
            percentage=100      # Route 100% of traffic
        )

    elif action == "adjust_retry_config":
        success = await self.tools.adjust_retry_config(
            max_retries=5,
            backoff_multiplier=1.5
        )

    elif action == "send_alert":
        success = await self.tools.send_alert(
            message="Critical Risk 0.70",
            severity="critical"
        )
```

---

### Stage 5: LEARN üìö

**Purpose:** Calculate reward and update policy

```python
async def _learn_node(self, state: AgentState) -> dict:
    # 1. Wait to measure impact
    await asyncio.sleep(2)

    # 2. Get new metrics
    new_metrics = await self.tools.get_current_metrics()

    # 3. Calculate reward
    success_gain = new_metrics["success_rate"] - old_metrics["success_rate"]

    action_cost = {
        "monitor": 0, "retry": 5, "send_alert": 10, "switch_gateway": 20
    }[learner_key]

    reward = (success_gain * 2.0) - latency_penalty - action_cost

    # 4. Update Policy Learner
    self.tools.ml.policy.update_policy(context, learner_key, reward)

    # 5. Store in memory if highly successful
    if success_gain > 5:
        await self.tools.store_memory({
            "anomaly_pattern": state["anomalies"],
            "intervention": state["intervention"],
            "outcome": "success",
            "improvement": success_gain
        })
```

---

## üß† ML Components

### 1. Anomaly Detector (Isolation Forest)

**Location:** `backend/app/ml/anomaly.py`

- Detects unusual patterns in metrics
- Faster than threshold-based detection
- Self-trains on historical data

### 2. Failure Predictor (XGBoost)

**Location:** `backend/app/ml/predictor.py`

- Predicts FUTURE failure probability
- Uses SHAP for explainability
- Enables proactive action

### 3. Policy Learner (Contextual Bandit)

**Location:** `backend/app/ml/predictor.py`

- Learns optimal action selection
- Updates Q-values using rewards
- Balances exploration vs exploitation

---

## üìä What Can Fail & How Agent Solves It

| Failure Scenario       | Detection              | Agent Response                 |
| ---------------------- | ---------------------- | ------------------------------ |
| **Bank Timeout**       | Latency spike + errors | Switch traffic to healthy bank |
| **Card Network Issue** | Method-specific errors | Increase monitoring, alert     |
| **Complete Outage**    | 95%+ failure rate      | Emergency reroute + alert      |
| **System Overload**    | Global error increase  | Adjust retry config            |

---

## üõ°Ô∏è Guardrails

| Action                | Risk Level | Auto-Approve? |
| --------------------- | ---------- | ------------- |
| `increase_monitoring` | LOW        | ‚úÖ Yes        |
| `adjust_retry_config` | LOW        | ‚úÖ Yes        |
| `send_alert`          | LOW        | ‚úÖ Yes        |
| `switch_gateway`      | HIGH       | ‚ö†Ô∏è Demo: auto |

---

## üèÜ Key Differentiators

1. **True Agent Loop** - Continuous observation, not one-shot
2. **Reinforcement Learning** - Policy improves via rewards
3. **Long-term Memory** - Remembers successful interventions
4. **Explainable ML** - SHAP values explain predictions
5. **Real-time UI** - WebSocket streams agent thoughts

---

_Team B042 | Veeral Saxena & Srishtee Varule | Taqneeq Hackathon 2026_
